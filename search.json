[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julian Granna, MSc.",
    "section": "",
    "text": "I am a university assistant and PhD candidate at the Department of Statistics, University of Innsbruck, Austria. My research interests include mainly the modeling of real estate prices including the computation of hedonic indices utilizing mainly Generalized Additive Models and Tree-Based Methods. I am however interested in other methods of Statistical and Machine Learning."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Julian Granna, MSc.",
    "section": "Education",
    "text": "Education\nUniversity of Innsbruck | Innsbruck, AT\nPhD in Economics and Statistics\nSept 2019 - ongoing\nUniversity of Innsbruck | Innsbruck, AT\nMSc in Economics\nOct 2015 - May 2019\nUniversity of Hanover | Hanover, DE\nBSc in Economics and Business Administration\nOct 2011 - May 2015"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Julian Granna, MSc.",
    "section": "Experience",
    "text": "Experience\nUniversity Assistant (PhD candidate)\nUniversity of Innsbruck\nSep 2019 – Present\nResearch Platform coordinator\nUniversity of Innsbruck\nJul 2017 – Mar 2019\nHomepage design and maintenance, organization of seminars, maintenance of working paper series, among others.\nExternal lecturer\nUniversity of Innsbruck\nMar 2017 – Jul 2018\nExternal lecturer in Statistical Data Analysis (Undergraduate level)."
  },
  {
    "objectID": "quantile_plots.html",
    "href": "quantile_plots.html",
    "title": "Quantile plots",
    "section": "",
    "text": "For each cluster, the animated graph shows the \\(2.5\\%\\), \\(5\\%\\), \\(25\\%\\), \\(50\\%\\), \\(75\\%\\), \\(95\\%\\), and \\(97.5\\%\\) quantiles."
  },
  {
    "objectID": "posts/commercial-fishing.html",
    "href": "posts/commercial-fishing.html",
    "title": "Commercial Fishing",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load('2021-06-08')"
  },
  {
    "objectID": "posts/commercial-fishing.html#tidy-tuesday",
    "href": "posts/commercial-fishing.html#tidy-tuesday",
    "title": "Commercial Fishing",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my seventh contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/commercial-fishing.html#commercial-fishing",
    "href": "posts/commercial-fishing.html#commercial-fishing",
    "title": "Commercial Fishing",
    "section": "Commercial Fishing",
    "text": "Commercial Fishing\nThis week’s data comes from Great Lakes Fishery Commission provided by (Baldwin, N.S., Saalfeld, R.W., Dochoda, M.R., Buettner, H.J., Eshenroder, R.L., and O’Gorman, R. 2018). It contains commercial fish catch data from the Great Lakes over a time horizon from 1867 to 2015.\n\nEvolution of fishing amounts of Chinook Salmon and Yellow Perch\nIn 1989, the first quagga mussels were discovered in the Erie Lake (Karatayev et al. 2014). They are considered an invasive species and originate from Ukraine in Eastern Europe. Since then, it has spread throughout all of the Big Lakes and their surrounding area. They contribute to cleansing of the water and thus to food scarcity for native fish such as the popular Chinook, or King Salmon. One of the known predators of the Quagga mussels is the Yellow Perch. I assume that it could have profited from the spread of the mussels.\nQuagga mussel population density reached its peak between 1998 and 2002. I am interested in whether the Chinook Salmon population has declined since the discovery of Quagga mussels in 1989 or around their population maximum between 1998 and 2002. Simultaneously, one could suspect an increase in the Yellow Perch population and hence their fishing rate.\nTo get a descriptive picture, I first filter out the fishing rates of the Yellow Perch and Chinook Salmon and aggregate the numbers over all lakes in each year:\n\n# filter out chinook salmon and yellow perch\nfishing <- fishing %>% filter(species == \"Yellow Perch\" | species == \"Chinook Salmon\") %>%\n  filter(year >= 1992)\n# get minimum time for each track\nsumfish <- aggregate(fishing$values ~ fishing$year + fishing$species, FUN = sum)\nnames(sumfish) <- c(\"year\", \"species\", \"value\")\n\nThen, I plot the amount of fish caught for the Chinook Salmon and the Yellow Perch, respectively:\n\n\n\n\n\nIt is apparent that the amount of King Salmon has decreased substantially over the years while the weight of caught Yellow Perch has increased, especially during the Quagga mussle population peak between 1998 and 2002. While the decrease in Chinook Salmon is generally considered to be (in large extent) by the large mussle population, it is of course questionable, whether it has led to increasing number of Yellow Perch. This would need much further investigation, but I nonetheless find it an interesting trend.\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/commercial-fishing.html#references",
    "href": "posts/commercial-fishing.html#references",
    "title": "Commercial Fishing",
    "section": "References",
    "text": "References\n\nggtext-package:\nhttps://wilkelab.org/ggtext/\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/"
  },
  {
    "objectID": "posts/scooby-doo.html",
    "href": "posts/scooby-doo.html",
    "title": "Scooby Doo Episodes",
    "section": "",
    "text": "This is my nineth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/scooby-doo.html#scooby-doo-episodes",
    "href": "posts/scooby-doo.html#scooby-doo-episodes",
    "title": "Scooby Doo Episodes",
    "section": "Scooby Doo Episodes",
    "text": "Scooby Doo Episodes\nThis week’s data comes from Kaggle and was aggregated by plummye. It contains every Scooby-Doo episode and the movies along with various other variables.\n\nWord Cloud for Episode Titles\nThe dataset is very large and one could spend a lot of time investigating it. I am interested in which words are used frequently in the episodes’ titles. To investigate this descriptively, I plot the frequency of the words in the titles as a word cloud using the wordcloud2-package. To make the word cloud take the shape of scooby, I provide a shape image containing the silhouette of Scooby, which was received here:\n\n# filter out uninteresting words and interpret \"scooby's\" as \"scooby\"\nscooby$title <- gsub(\"Scooby's\", \"scooby\", scooby$title) \nscooby$title <- gsub(\"and\", \" \", scooby$title) \nscooby$title <- gsub(\"The\", \" \", scooby$title) \nscooby$title <- gsub(\"the\", \" \", scooby$title) \nscooby$title <- gsub(\"for\", \" \", scooby$title) \nscooby$title <- gsub(\"from\", \" \", scooby$title) \n# convert to corpus:\ndocs <- Corpus(VectorSource(scooby$title))\n# Convert the text to lower case\ndocs <- tm_map(docs, content_transformer(tolower))\n# Remove numbers\ndocs <- tm_map(docs, removeNumbers)\n# Remove punctuations, but keep apostrophes (e.g. don't, not dont)\ndocs <- tm_map(docs, removePunctuation, preserve_intra_word_contractions = TRUE)\n# Eliminate extra white spaces\ndocs <- tm_map(docs, stripWhitespace)\n\n# create term document matrix\ndtm <- TermDocumentMatrix(docs)\nm <- as.matrix(dtm)\nv <- sort(rowSums(m),decreasing=TRUE)\nd <- data.frame(word = names(v),freq=v)\n# reduce frequeny, so that words fit into graphics device (not ideal!)\nd$freq[1:2] <- 30\n# use different colors\ncols <- hcl.colors(982, palette = \"Green-Orange\")\nset.seed(17721)\nw <- wordcloud2(d, figPath = \"https://github.com/jgranna/tidytuesday/blob/main/2021-07-13/_images/scooby.jpg\", size = 1, backgroundColor=\"black\", minSize = 1, color = cols)\n\n\n\n\nScooby Doo\n\n\nIt is apparent, that “scoobydoo” is used frequently in the episodes’ titles. But also the words “mystery”, “night”, “ghost”, etc. occur quite often.\nOne could spend much more time to improve the image. Also, the plotting only worked for me in the browser and thus I could not export the image in a “nice way”, but had to export it manually from the browser, which is of course not ideal.\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/scooby-doo.html#references",
    "href": "posts/scooby-doo.html#references",
    "title": "Scooby Doo Episodes",
    "section": "References",
    "text": "References\n\ncolorspace-package:\nhttps://colorspace.r-forge.r-project.org/index.html\ntm-package:\nhttps://tm.r-forge.r-project.org\nwordcloud2-package:\nhttps://github.com/Lchiffon/wordcloud2"
  },
  {
    "objectID": "posts/lego-sets.html",
    "href": "posts/lego-sets.html",
    "title": "LEGO Sets",
    "section": "",
    "text": "# data <- tidytuesdayR::tt_load(\"2022-09-06\")\ninventories <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-06/inventories.csv.gz')\ninventory_sets <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-06/inventory_sets.csv.gz')\nsets <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-06/sets.csv.gz')\nlego <- left_join(inventories, inventory_sets, by = \"set_num\") |>\n  left_join(sets, by = \"set_num\")"
  },
  {
    "objectID": "posts/lego-sets.html#tidy-tuesday",
    "href": "posts/lego-sets.html#tidy-tuesday",
    "title": "LEGO Sets",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my eleventh contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom.\n\n\nLoading required package: coda\n\n\nLoading required package: colorspace\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n\n\nAttaching package: 'bamlss'\n\n\nThe following object is masked from 'package:mgcv':\n\n    smooth.construct\n\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb"
  },
  {
    "objectID": "posts/lego-sets.html#lego-sets",
    "href": "posts/lego-sets.html#lego-sets",
    "title": "LEGO Sets",
    "section": "LEGO sets",
    "text": "LEGO sets\nThis week’s data comes from rebrickable courtesy of Georgios Karamanis. The dataset contains 35,023 observations of LEGO sets over a time horizon from 1949 to 2022. This is a quick look at the variables (from the Tidy Tuesday vignette):\n\nVariable description\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nvariable\n\n\nversion\ndouble\nvariable\n\n\nset_num\ncharacter\nvariable\n\n\ninventory_id\ndouble\nvariable\n\n\nname\ncharacter\nvariable\n\n\nquantity\ndouble\nvariable\n\n\nyear\ndouble\nvariable\n\n\ntheme_id\ndouble\nvariable\n\n\nnum_parts\ndouble\nvariable\n\n\nimg_url\ncharacter\nvariable\n\n\n\nThe descriptions are in this case not very informative.\n\nGoals\nThe data offers a lot to visualize. One could of course, for example, use the ggpattern package again to include nice LEGO backgrounds. This week, however, I want to provide a quick example of Bayesian distributional regression, where a whole distribution is fit to the data, instead of, e.g. just the mean. For this purpose, I will be using the bamlss package. I want to pay a special focus on how the distribution of the number of parts in LEGO sets has changed over time.\n\n\nDescriptives: Density plots\nTo gain a first overview, I plot the corresponding distributions over time. Here, I actually decided to use the ggpattern package to plot the densities in a nice LEGO style. To keep things cleaner, I first cut the complete time horizon into strips:\n\n# data cleaning first, also remove entries w/ less than 10 # of parts to regard actual sets\nlego <- subset(lego, !is.na(year) & !is.na(num_parts) & num_parts >= 10)\nlego$time_cut <- cut(lego$year, breaks = c(1949, 1975, 2010, 2022), include.lowest = TRUE)\nlevels(lego$time_cut) <- c(\"1949 - 1975\", \"1976 - 2010\", \"2011 - 2022\")\n\n# actual density plot\nlegend.title = \"years\"\nplot1 <- ggplot(lego) +\n  theme_minimal() +\n  geom_density_pattern(aes(x = num_parts, fill = time_cut, color = time_cut, \n                           pattern_fill = time_cut, pattern_color = time_cut),\n                       pattern = \"circle\",\n                       pattern_angle = 90, \n                       pattern_density = 0.5) +\n  scale_pattern_fill_manual(values = c(\"#a80005\", \"#0031c3\", \"#006906\")) +\n  scale_pattern_color_manual(values = c(\"#a80005\", \"#0031c3\", \"#006906\")) +\n  scale_fill_manual(values = c(\"#e40004\", \"#0044ff\", \"#008607\")) +\n  scale_color_manual(values = c(\"#e40004\", \"#0044ff\", \"#008607\")) +\n  scale_x_log10() +\n  labs(\n    fill = legend.title,\n    color = legend.title, \n    pattern_fill = legend.title,\n    pattern_color = legend.title,\n    x = \"number of parts\"\n  ) +\n  theme(\n    legend.position = c(0.875, 0.8),\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    axis.title.x = element_text(face = \"bold\", color = \"white\", family = \"kalam\", size = 18, margin = margin(8, 0, 0, 0)),\n    axis.title.y = element_text(face = \"bold\", color = \"white\", angle = 90, family = \"kalam\", size = 18, margin = margin(0, -20, 0, 0)),\n    axis.text.x = element_text(color = \"white\", family = \"kalam\"),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(.2, \"cm\"),\n    legend.background = element_rect(fill = \"transparent\", color = NA),\n    legend.text = element_text(color = \"white\", size = 15, family = \"kalam\"),\n    legend.title = element_text(color = \"white\", family = \"kalam\", hjust = 0.5, size = 25),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  ) +\n  theme(plot.margin = margin(30, 30, 8, 7))\nplot1\n\n\n\n\nFrom the plot it seems that over the years, the number of parts in LEGO sets has increased both with regard to its expected value and its variance. There is a gradual shift from the left to the right. This could of course be part of some diversification strategy of LEGO (of course, this is just speculation).\n\n\nBayesian Distributional Regression\nNow, as explained earlier, I employ the bamlss package to model the number of parts’ distribution. Of course, it would be good to first look for an appropriate distribution to fit to the data. I don’t undergo this procedure in this case, however.\n\n\nAICc 51042.93 logPost -25593.4 logLik -25493.6 edf 27.763 eps 0.8122 iteration   1\nAICc 50998.71 logPost -25578.0 logLik -25468.6 edf 30.675 eps 0.1131 iteration   2\nAICc 50997.60 logPost -25578.0 logLik -25467.7 edf 30.967 eps 0.0125 iteration   3\nAICc 50997.53 logPost -25578.1 logLik -25467.7 edf 30.988 eps 0.0008 iteration   4\nAICc 50997.52 logPost -25578.1 logLik -25467.7 edf 30.990 eps 0.0000 iteration   5\nAICc 50997.52 logPost -25578.1 logLik -25467.7 edf 30.990 eps 0.0000 iteration   5\nelapsed time:  1.06sec\nStarting the sampler...\n\n|                    |   0% 24.51sec\n|*                   |   5% 18.83sec  0.99sec\n|**                  |  10% 18.13sec  2.01sec\n|***                 |  15% 16.35sec  2.89sec\n|****                |  20% 15.55sec  3.89sec\n|*****               |  25% 14.86sec  4.95sec\n|******              |  30% 14.03sec  6.01sec\n|*******             |  35% 13.40sec  7.22sec\n|********            |  40% 12.42sec  8.28sec\n|*********           |  45% 11.42sec  9.35sec\n|**********          |  50% 10.41sec 10.41sec\n|***********         |  55%  9.40sec 11.49sec\n|************        |  60%  8.47sec 12.71sec\n|*************       |  65%  7.42sec 13.77sec\n|**************      |  70%  6.36sec 14.84sec\n|***************     |  75%  5.30sec 15.91sec\n|****************    |  80%  4.24sec 16.98sec\n|*****************   |  85%  3.21sec 18.19sec\n|******************  |  90%  2.14sec 19.25sec\n|******************* |  95%  1.07sec 20.30sec\n|********************| 100%  0.00sec 21.36sec\n\n\nA first glance at the model:\n\npar(mfrow = c(1, 2))\nplot(m)\n\n\n\n\nIt is interesting to see that both the mean and variance (or standard deviation) of the number of parts increases gradually with time. The strategy to release sets with more parts and increasing the variance at the same time thus does not seem to be a sudden decision, but instead a strategy that has been adjusted gradually. This is of course just a first look at the data and conclusions have to be regarded cautiously.\n\n\nBayesian Plots\nNow, I finally take the first plot and make a visually (somewhat) nice plot out of it.\n\n# to speed things up, don't use all samples, but just 200\nsmpl <- sample(1000, 100)\n\n# get predictions from sampled coefficients\np.mu <- predict(m, model = \"mu\", type =\"parameter\", FUN = function(x) {x})\n\n# bias correction: mean of exp(residuals)\nh_0 <- mean(exp(residuals(m)))\np.mu <- lapply(smpl, function(x) h_0 * exp(p.mu[[x]]))\np.final.mu <- h_0 * exp(predict(m, model = \"mu\", type = \"parameter\"))\n\n# convert into tidy format for ggplot\nplot.tab.mu <- do.call(cbind, p.mu)\nplot.tab.mu <- cbind(lego[order(lego$year), ]$year, plot.tab.mu)\nplot.tab.mu <- data.frame(plot.tab.mu)\nplot.tab.mu <- plot.tab.mu %>% pivot_longer(X2:X101, values_to = \"predictions\")\n\n# plot mu\nfont_add_google(\"Pangolin\", \"kalam\")\np2 <- ggplot(subset(plot.tab.mu, name %in% paste0(\"X\", 2:101)), aes(x = X1, y = predictions, group = name)) +\n  geom_line(col = alpha(\"red\", 0.14)) +\n  theme_void() +\n  scale_y_continuous(breaks = seq(35, 315, 50)) +\n  scale_x_continuous(breaks = c(1949, seq(1960, 2020, 20))) +\n  labs(x = \"year\", y = \"mu\") +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.grid.major.x = element_line(color = alpha(\"white\", 0.5), size = 0.4),\n    panel.grid.major.y = element_line(color = alpha(\"white\", 0.8), size = 0.8),\n    axis.title.x = element_text(face = \"bold\", color = \"white\", family = \"kalam\", size = 18, margin = margin(8, 0, 5, 0)),\n    axis.title.y = element_text(face = \"bold\", color = \"white\", angle = 90, family = \"kalam\", size = 18, margin = margin(0, 8, 0, 0)),\n    axis.text = element_text(color = \"white\", family = \"kalam\"),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(.2, \"cm\")\n  ) +\n  coord_cartesian(ylim = c(35, 315)) +\n  theme(plot.margin = margin(8, 8, 30, 30))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np2\n\n\n\n\n\n# get predictions for sigma\np.sigma <- predict(m, model = \"sigma\", type =\"parameter\", FUN = function(x) {x})\np.final.sigma <- predict(m, model = \"sigma\", type = \"parameter\")\np.final.sigma <- exp(p.final.sigma)\n\n# transformation of sigma\np.sigma <- lapply(smpl, function(x) exp(p.sigma[[x]]))\n\n# convert into tidy format for ggplot\nplot.tab.sigma <- do.call(cbind, p.sigma)\nplot.tab.sigma <- cbind(lego[order(lego$year), ]$year, plot.tab.sigma)\nplot.tab.sigma <- data.frame(plot.tab.sigma)\nplot.tab.sigma <- plot.tab.sigma %>% pivot_longer(X2:X101, values_to = \"predictions\")\n\n# plot mu\np3 <- ggplot(subset(plot.tab.sigma, name %in% paste0(\"X\", 2:101)), aes(x = X1, y = predictions, group = name)) +\n  theme_void() +\n  scale_y_continuous(breaks = 2:5) +\n  scale_x_continuous(breaks = c(1949, seq(1960, 2020, 20))) +\n  geom_line(col = alpha(\"green\", 0.14)) +\n  labs(x = \"year\", y = \"sigma\") +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.grid.major.x = element_line(color = alpha(\"white\", 0.5), size = 0.4),\n    panel.grid.major.y = element_line(color = alpha(\"white\", 0.8), size = 0.8),\n    axis.title.x = element_text(face = \"bold\", color = \"white\", family = \"kalam\", size = 18, margin = margin(8, 0, 0, 0)),\n    axis.title.y = element_text(face = \"bold\", color = \"white\", angle = 90, family = \"kalam\", size = 18, margin = margin(0, 8, 0, 0)),\n    axis.text = element_text(color = \"white\", family = \"kalam\"),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(.2, \"cm\")\n  ) +\n  coord_cartesian(ylim = c(1.7, 5.1)) +\n  theme(plot.margin = margin(8, 30, 30, 8))\np3\n\n\n\n\n\n\nFinal Plot\nNow, I add everything together with the patchwork package.\n\ntheme_set(theme(text = element_text(family = \"kalam\")))\nshowtext_auto()\nlogo <- ggplot() +                      # Draw ggplot2 plot with text only\n    theme_void() +\n    geom_rect(aes(xmin = 0, xmax = 1.25, ymin = 0.8, ymax = 1.3), fill = \"red\") +\n    geom_shadowtext(aes(x = 1, y = 1, label = \"LEGO\", family = \"kalam\"), bg.color = \"yellow\", bg.r = 0.18, size = 22, fontface = \"italic\") +\n  geom_shadowtext(aes(x = 1, y = 1, label = \"LEGO\", family = \"kalam\"), bg.color = \"black\", size = 22,\n                  fontface = \"italic\") +\n  geom_text(\n    aes(x = 1.43, y = 1, label = \"Sets\"), family = \"kalam\", size = 24, fontface = \"bold\", color = \"white\") + \n  geom_text(aes(x = 0.75, y = 0.4, label = \"Graphs show the distribution of the number of parts  in LEGO sets. \\nThe panel on the right shows the corresponding density plot for three different \\ntime periods.\\nThe bottom two panels refer to a Bayesian distributional regression model, \\nwhere both log(mu) and sigma of a normal distribution are modeled using \\npenalized splines within the bamlss package. Apparently, LEGO has adjusted its \\nstrategy to not only increase the mean number of parts of each set, but also its \\nvariance meaning that nowadays, both, sets containing only few parts, but \\nalso sets containing a large number of parts, are available to customers. \"), size = 6, color = \"white\", family = \"kalam\", lineheight = 0.6, hjust = 0) +\n  coord_cartesian(ylim = c(0,1.15), xlim = c(0.8, 2)) + \n  theme(panel.background = element_rect(fill = \"transparent\", color = NA),\n        plot.background = element_rect(fill = \"transparent\", color = NA)) +\n  theme(plot.margin = margin(30, 0, 8, 0))\nlogo\n\n\n\n\nNow, I can use the patchwork package to plug all plots together\n\n(logo + plot1) / (p2 + p3) &\n  plot_annotation(theme = theme(plot.background = element_rect(color = \"black\", fill = \"black\")))\n\n\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/lego-sets.html#references",
    "href": "posts/lego-sets.html#references",
    "title": "LEGO Sets",
    "section": "References",
    "text": "References\n\ncolorspace-package:\nhttps://colorspace.r-forge.r-project.org/index.html\nbamlss-package:\nhttp://bamlss.org/\nparallel-package:\nhttps://parallelly.futureverse.org/\nshadowtext-package:\nhttps://github.com/GuangchuangYu/shadowtext/\nggpattern-package:\nhttps://coolbutuseless.github.io/package/ggpattern/index.html\nshowtext-package:\nhttps://cran.r-project.org/web/packages/showtext/index.html\nggtext--package:\nhttps://wilkelab.org/ggtext/\npatchwork--package:\nhttps://patchwork.data-imaginist.com/\ntidyr--package:\nhttps://tidyr.tidyverse.org/\n`rlist``--package:\nhttps://renkun-ken.github.io/rlist/"
  },
  {
    "objectID": "posts/canadian_heatwave.html",
    "href": "posts/canadian_heatwave.html",
    "title": "Canadian Heatwave",
    "section": "",
    "text": "Regarding the exceptional heatwave in Canada in June 2021, I decided to pause TidyTuesday for a week and instead visualize minimum and maximum temperatures in Portland between 1938 and 2021. This work is inspired by a post by the New York Times and a tweet by Cedric Scherer."
  },
  {
    "objectID": "posts/canadian_heatwave.html#scatter-plot-of-maximum-temperatures-in-portland-canada",
    "href": "posts/canadian_heatwave.html#scatter-plot-of-maximum-temperatures-in-portland-canada",
    "title": "Canadian Heatwave",
    "section": "Scatter Plot of Maximum Temperatures in Portland, Canada",
    "text": "Scatter Plot of Maximum Temperatures in Portland, Canada\nJune 26-28, 2021, were the hottest days ever recorded in Portland, Canada, reaching a maximum of 116 degrees Fahrenheit (roughly 46.7 degrees Celsius). To show how unusually hot these days are in the last roughly 80 years, I provide a scatter plot, where the outlier temperatures are easily identifyable. I choose a plot on a polar axis and to indicate possible time trends, I color the points corresponding to their respective 20-year period. I also color the main y-grid lines for slightly improved interpretability.\n\n\n\n\np <- df %>%\n  ggplot(aes(x = yday, y = tmax)) +\n  geom_point(aes(color = years), alpha = 0.4, size = 0.4) +\n  scale_color_discrete_sequential(palette = \"Plasma\") +\n  scale_x_continuous(breaks = months$yday, labels = months$label, expand = c(.001, .001)) + \n  scale_y_continuous(breaks = c(30, 50, 70, 90, 110), labels = c('30', '50', '70', '90', '110'), expand = c(.001, .001)) + \n  coord_polar() +\n  labs(\n    y = expression(paste(\"daily temperature (\", degree ~ F, \")\"))\n  ) +\n  guides(\n    color = guide_legend(override.aes = list(size = 4, alpha = 1), nrow = 1, title.position = \"bottom\")\n  )\np + \n  labs(\n    caption = \"**Data:** NOAA | **Visualization:** @jgranna\",\n    title = \"**Daily Maximum Temperatures in Portland, 1939 - 2021**\"\n  )\n\n\n\n#ggforce::geom_mark_ellipse(  #--- not included; optional ellipse marking ---#\n#  aes(fill = new_record, label = new_record, filter = new_record != \"\", description = desc), \n#  alpha = 0.5,color = \"00000000\", label.colour = \"grey20\", con.colour = \"grey20\",\n#  expand = unit(0, \"mm\"), con.cap = 5, show.legend = FALSE, label.buffer = unit(25, 'mm'),\n#  label.fontsize = c(25, 20), label.family = \"osans\"\n#)"
  },
  {
    "objectID": "posts/canadian_heatwave.html#smoothed-maximum-temperatures",
    "href": "posts/canadian_heatwave.html#smoothed-maximum-temperatures",
    "title": "Canadian Heatwave",
    "section": "Smoothed maximum temperatures",
    "text": "Smoothed maximum temperatures\nThe three days in June 2021 are clearly visible at the bottom of the graph, close to July. However, possibly due to the mass of points, a time trend is not easily identifyable. In order to improve this, I plot smoothed functions of the 20-years periods. I choose a GAM smoother, which I chose more or less arbitrarily. However, this is sufficient to be able to identify a time trend. That is, maximum temperatures are generally increasing in Portland, which is of course little surprising."
  },
  {
    "objectID": "posts/canadian_heatwave.html#combine-both-graphs",
    "href": "posts/canadian_heatwave.html#combine-both-graphs",
    "title": "Canadian Heatwave",
    "section": "Combine both graphs",
    "text": "Combine both graphs\nFinally, for a better overview (and practice), I combine both plots using the patchwork- package:\n\nlibrary(patchwork)\np + theme(legend.position = \"none\") + p2 + theme(axis.text.y = element_text(margin = margin(0,6,0,4)), axis.title.y = element_blank()) + scale_y_continuous(position = \"right\") +\n  plot_layout(guides = \"collect\") +\n  plot_annotation(\n    caption = \"**Data:** NOAA | **Visualization:** @jgranna\",\n    subtitle = \"The left plot shows a scatter plot of maximum temperatures in Portland. The right plot gives the corresponding GAM smooths which makes it easier to visually identify a time trend in the data.\", \n    title = \"**Daily Maximum Temperatures in Portland, 1939 - 2021**\"\n  ) &\n  theme(plot.caption = element_markdown(margin = margin(0,3,0,0))) \n\n\n\n\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/canadian_heatwave.html#used-packages",
    "href": "posts/canadian_heatwave.html#used-packages",
    "title": "Canadian Heatwave",
    "section": "Used Packages",
    "text": "Used Packages\n\ncolorspace-package:\nhttps://colorspace.r-forge.r-project.org/articles/colorspace.html\nggtext-package:\nhttps://wilkelab.org/ggtext/\npatchwork-package:\nhttps://patchwork.data-imaginist.com/index.html\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/"
  },
  {
    "objectID": "posts/deforestation.html",
    "href": "posts/deforestation.html",
    "title": "Deforestation",
    "section": "",
    "text": "# install.packages(\"remotes\")\n# remotes::install_github(\"davidsjoberg/ggstream\")\n\nlibrary(tidytuesdayR)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ggstream)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:ggthemes':\n\n    theme_map\n\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nlibrary(ggtext)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(gitcreds)\nfont_add_google(\"Dosis\", \"dosis\")\nknitr::opts_chunk$set(fig.width=9.5, fig.height=5)"
  },
  {
    "objectID": "posts/deforestation.html#tidy-tuesday",
    "href": "posts/deforestation.html#tidy-tuesday",
    "title": "Deforestation",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my first contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage or check out their GitHub repository."
  },
  {
    "objectID": "posts/deforestation.html#worldwide-soy-production-over-time",
    "href": "posts/deforestation.html#worldwide-soy-production-over-time",
    "title": "Deforestation",
    "section": "Worldwide soy production over time",
    "text": "Worldwide soy production over time\nStreamgraph for soy production per continent:\n\n\n\n\n\nThe global production of soy has increased rapidly over the last 60 years. Europe’s share in global production is stagnating and stable since around 1980. Growth in soy production mainly stems from production increases in Asia and South America."
  },
  {
    "objectID": "posts/deforestation.html#brazilian-forest-loss",
    "href": "posts/deforestation.html#brazilian-forest-loss",
    "title": "Deforestation",
    "section": "Brazilian Forest Loss",
    "text": "Brazilian Forest Loss\nTo get a picture of total forest loss next to forest loss and its causes, it is useful to again regard the corresponding streamgraph:\n\n\n\n\n\nThe graph shows that total forest loss between 2001 and 2004 is much higher than in more recent years. This corresponds to evolution of forest losses in other countries with increasing development. This trend however, could (temporarily) reverse in more recent years due to less regulatory restrictions. Pasture is the major cause of forest loss in Brazil. This is true over the whole time horizon. However, the share has been decreasing in recent years. Other causes, like fire, roads, or plantations including palm oil, play a much smaller - but also substantial - role."
  },
  {
    "objectID": "posts/deforestation.html#share-of-global-forest-area-per-continent-over-time",
    "href": "posts/deforestation.html#share-of-global-forest-area-per-continent-over-time",
    "title": "Deforestation",
    "section": "Share of global forest area per continent over time",
    "text": "Share of global forest area per continent over time\n\n\n\nVisualize change in change of global forest share over time:\n\n\n\n\n\nThe share of each continents’ forest area of the world’s total forest area has not changed substantially over the last 30 years.\n\nFull R code available at https://github.com/jgranna/tidytuesday"
  },
  {
    "objectID": "posts/ceo-departures.html",
    "href": "posts/ceo-departures.html",
    "title": "CEO departures from S&P 1500",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load(\"2021-04-27\")"
  },
  {
    "objectID": "posts/ceo-departures.html#tidy-tuesday",
    "href": "posts/ceo-departures.html#tidy-tuesday",
    "title": "CEO departures from S&P 1500",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my fourth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/ceo-departures.html#this-weeks-data",
    "href": "posts/ceo-departures.html#this-weeks-data",
    "title": "CEO departures from S&P 1500",
    "section": "This week’s data",
    "text": "This week’s data\nThis week’s data concerns CEO departures from S&P 1500 firms between 1991 and 2019 by Gentry et al. 2021 via DatalsPlural. Among other variables, the data contains the corresponding company names, the year of the departure, and the full name of the departed CEO."
  },
  {
    "objectID": "posts/ceo-departures.html#diversity-in-the-data",
    "href": "posts/ceo-departures.html#diversity-in-the-data",
    "title": "CEO departures from S&P 1500",
    "section": "Diversity in the data",
    "text": "Diversity in the data\nThere are several possibilities to track diversity in the CEOs of S&P 1500 firms. Again, the aim of this post is not to provide scientifically valid analyses, but rather to show the possibilities of ggplot and other available R packages and give a short (descriptive) overview over the data.\nThe first thing that struck me when looking at the data was that the top 7 occuring first names of departed CEOs account for close to 30% of the CEOs:\n\nlength(aceos$fnames[aceos$fnames %in% top_names]) / nrow(aceos)\n\n[1] 0.2856838\n\ntop_names\n\n[1] \"Thomas\"  \"William\" \"Robert\"  \"James\"   \"Michael\" \"David\"   \"John\"   \n\n\nThis of course is a strong indicator that the whole spectrum of departed CEOs is not really diverse. Moreover, all names are male (when regarded on a state-imposed binary gender scale) and are likely to be not associated with people belonging to minorities. As one could expect, the departed CEOs do not represent the spectrum of different ethnicities (and genders) of the society.\nTo emphasize the importance of just a few names for the group of S&P 1500 (departed) CEOs, I decided to make a graph using ggbump indicating a “race” between the 5 most occuring names. The following graph thereby regards the cumulated number of names starting from 2010. I chose 2010 to 2019 as a time window simply for visualization reasons, which is also the case for the number of regarded names:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\nEvolution of diversity over time\nWhen starting my analysis, I was also interested in not only how diverse the departed CEOs are over the complete time horizon, but also whether the diversity at least increases over time.\nAn economic approach to measure unequal (i. e. undiverse) allocations in general is to assess the Gini coefficient. The Gini coefficient is standardized to have values between 0 and 1, where 0 refers to perfect equality in the allocation (i. e. each name occuring in exactly the same frequency) and 1 refers to only a single first name accounting for all departed CEOs.\nAlthough it is obvious that this methodology is flawed in this context, I hoped to maybe gain some interesting insights by simply tracking the Gini coefficient over time:\n\n\n\n\n\nBut it turns out that the index is instead driven by the total number of names of departed CEOs:\n\n\n\n\n\nThus, one would need more sophisticated measures to track the evolution of diversity in the S&P 1500. I believe that there would be room for improvement regarding the use of the Gini coefficient, but I choose to discontinue my analysis at this point due to time constrainment."
  },
  {
    "objectID": "posts/ceo-departures.html#references",
    "href": "posts/ceo-departures.html#references",
    "title": "CEO departures from S&P 1500",
    "section": "References",
    "text": "References\n\nRichard J. Gentry, Joseph S. Harrison, Timothy J. Quigley, Steven Boivie, 2021. A database of CEO turnover and dismissal in S&P 1500 firms. 2000–2018, https://doi.org/10.1002/smj.3278.\nggbump-package:\nhttps://github.com/davidsjoberg/ggbump\nggtext-package:\nhttps://wilkelab.org/ggtext/\nineq-package:\nhttps://cran.r-project.org/package=ineq\npbapply-package:\nhttps://github.com/psolymos/pbapply\nshowtext-package:\nhttps://github.com/yixuan/showtext\nstringr-package:\nhttps://stringr.tidyverse.org/\ntidyverse-package:\nhttps://www.tidyverse.org/\n\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/water-sources.html",
    "href": "posts/water-sources.html",
    "title": "Water Sources",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load(\"2021-05-04\")"
  },
  {
    "objectID": "posts/water-sources.html#tidy-tuesday",
    "href": "posts/water-sources.html#tidy-tuesday",
    "title": "Water Sources",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my fourth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/water-sources.html#water-sources",
    "href": "posts/water-sources.html#water-sources",
    "title": "Water Sources",
    "section": "Water Sources",
    "text": "Water Sources\nThis week’s data comes from Water Point Data Exchange. It is a smaller sub-dataset from the original data to include mainly African data.\n\nUsing this data and advanced GIS and machine learning analysis, several decision support tools were built. Designed in partnership with governments and data scientists, these tools provide concrete insights, like which water point to send a technician to rehabilitate next to reach the most people.\n– Katy Sill, Adam Kariv\n\n\nScatterplot of the data\nA scatterplot of the data shows the locational distribution of the observations. Arrows indicate countries with a relatively high water source density.\n\n\n\n\n\n\nType and Status of Water sources\nFurther interesting insights yields a bar plot of the types of water sources and an indicator of functionality:\n\n\n\n\nBoreholes are by far the most important source of water in the regarded countries. There is however a relatively large fraction of malfunctioning water sources as can be seen in the plot."
  },
  {
    "objectID": "posts/water-sources.html#references",
    "href": "posts/water-sources.html#references",
    "title": "Water Sources",
    "section": "References",
    "text": "References\n\nggtext-package:\nhttps://wilkelab.org/ggtext/\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/\nrnaturalearth-package:\nhttps://docs.ropensci.org/rnaturalearth/\nrnaturalearthdata-package:\nhttps://docs.ropensci.org/rnaturalearthdata/\nggspatial-package:\nhttps://paleolimbot.github.io/ggspatial/\nsf-package:\nhttps://r-spatial.github.io/sf/\ndata.table-package:\nhttps://rdatatable.gitlab.io/data.table/\n\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/pell_awards.html",
    "href": "posts/pell_awards.html",
    "title": "Pell Awards",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load(\"2022-08-30\")"
  },
  {
    "objectID": "posts/pell_awards.html#tidy-tuesday",
    "href": "posts/pell_awards.html#tidy-tuesday",
    "title": "Pell Awards",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my tenth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom.\nNote: This is the first post I generate using Quarto, ‘an open-source scientific and technical publishing system built on Pandoc’. I will use it frequently in my following posts.\n\nlibrary(colorspace)\nlibrary(knitr)\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(ggtext)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n\nAttaching package: 'rnaturalearthdata'\n\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(ggpattern)\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb"
  },
  {
    "objectID": "posts/pell_awards.html#pell-grants",
    "href": "posts/pell_awards.html#pell-grants",
    "title": "Pell Awards",
    "section": "Pell Grants",
    "text": "Pell Grants\nThis week’s data comes from US Department of Education. It is also available via the Pell Grant on CRAN. More details about the data can also be found in the package’s vignette. It contains 100474 observations of six variables (state, award, recipient, name, session, year). This is a quick look at the variables and their meaning (from the Tidy Tuesday vignette):\n\nVariable description\n\n\nvariable\nclass\ndescription\n\n\n\n\nSTATE\ninteger\nState shortcode\n\n\nAWARD\ndouble\nAward amount in USD\n\n\nRECIPIENT\ndouble\nTotal number of recipients by year, name\n\n\nNAME\ninteger\nName of college / university\n\n\nSESSION\ninteger\nSession group\n\n\nYEAR\ninteger\nYear\n\n\n\n\nHighest number of awards per university\nTo start out, I take a quick look at the universities who received the highest (and lowest) total amount of grants over the complete time horizon (1999 - 2017):\n\npell <- na.omit(pell)\nkable(\n  head(sort(tapply(pell$AWARD, pell$NAME, sum), decreasing = TRUE)),\n      col.names = c(\"Total grant money received\")\n)\n\n\n\n\n\n\n\n\n\nTotal grant money received\n\n\n\n\nUniversity of Phoenix\n7949271992\n\n\nAshford University\n1940355376\n\n\nCity University of New York Central Office\n1885270659\n\n\nItt Technical Institute\n1830449269\n\n\nDevry University\n1817806084\n\n\nKaplan University\n1531089195\n\n\n\n\n\nI use the kable() function from the knitr package in order to directly convert the R output to a table that can be displayed on this website. There are of course other possible ways to summarize the data, and some applicants prefer functions from the tidyverse package. I however, do not.\n\nkable(\n  head(sort(tapply(pell$AWARD, pell$NAME, sum), decreasing = FALSE)),\n      col.names = c(\"Total grant money received\")\n)\n\n\n\n\n\n\n\n\n\nTotal grant money received\n\n\n\n\nBank Street College of Education\n0\n\n\nBilingual Education Institute\n0\n\n\nBriarcliffe College - Patchogue\n0\n\n\nClaremont Graduate University\n0\n\n\nCuyahoga Community College - District Office\n0\n\n\nEastern New Mexico University - Rowsell\n0\n\n\n\n\n\nStudents from the University of Phoenix have received the most fundings from the Pell grant over the whole time horizon, totalling up to almost 7 billion USD from 1999-2017. Some universities / colleges, like Claremont Graduate University, have received no Pell Grants in the regarded horizon.\n\n\nDistribution of grants over the states\nAnother interesting aspect is the distribution of grants over the states in the US. I look at this aspect using a map generated again with the sf package. A new package I just discovered is the ggpattern package. I want to try this package and use it to color the background behind the actual map.\n\n\nRetrieving data for the year 2021\n\n\n\n# actual plot\ntheme_set(theme(text = element_text(family = \"osans\", size = 25)))\nshowtext_auto()\nggplot(a) + \n  geom_rect_pattern(aes(xmin = -3000000, xmax =3000000, ymin = -3000000, ymax = 900000), pattern = \"magick\",\n                    fill = \"#dcf1ff\", pattern_type = 'gray15', pattern_scale = 2, \n                    pattern_colour = \"#007ed3\", pattern_alpha = 0.002, pattern_angle = 40) +\n  geom_sf(aes(fill = amount)) +\n  scale_fill_binned_sequential(palette = \"Blue-Yellow\") +\n  coord_sf(crs = st_crs(2163), xlim = c(-2500000, 2500000), ylim = c(-2300000, \n         730000)) +\n  labs(\n    title = bquote(bold(\"Pell Awards Granted in the US\")),\n    subtitle = \"from 1999 to 2017\",\n    fill = bquote(bold(\"Grant amount\")~\" (in billion USD)\"),\n    caption = \"@jugRanna<br>**Source:** US Department of Education\"\n  ) +\n  theme(\n    plot.title = element_text(family = \"qwitcher\", size = 55, lineheight = 0, margin = margin(1, 0, 0.1, 1), vjust = -0.7),\n    plot.subtitle = element_text(family = \"qwitcher\", size = 45, lineheight = 0.35, vjust = 0.4),\n    legend.position = \"bottom\",\n    legend.background = element_rect(fill = '#ebf1f5', color = '#ebf1f5'),\n    panel.background = element_rect(fill = 'white', color = 'black'),\n    plot.background = element_rect(fill = \"#ebf1f5\", color = \"#ebf1f5\"),\n    plot.title.position = \"plot\",\n    panel.border = element_rect(color = \"black\", fill = NA, size = 2),\n    plot.caption = element_markdown(color = \"gray60\", lineheight = 0.35, hjust = 1, size = 20),\n    plot.caption.position = \"plot\"\n  ) +\n  guides(fill = guide_colorsteps(title.position = 'bottom', even.steps = TRUE,\n                                 barwidth = 20, barheight = 0.5,\n                                  title.hjust = .5)) \n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nIt is apparant that students in California have received the most money from the Pell Grant, more than 40 billion USD between 1999 and 2017. A close look at the number of universities / colleges reveals why that is the case (or at least part of the reason):\n\nkable(\n  head(\n    sort(table(pell$STATE), decreasing = TRUE)\n  )\n)\n\n\n\n\nVar1\nFreq\n\n\n\n\nCA\n9593\n\n\nNY\n6736\n\n\nTX\n5482\n\n\nPA\n5453\n\n\nFL\n4744\n\n\nOH\n4570\n\n\n\n\n\nCalifornia has by far the most universities / colleges in the country. Other states that show higher numbers in schools also receive more money from the Pell Grant.\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/pell_awards.html#references",
    "href": "posts/pell_awards.html#references",
    "title": "Pell Awards",
    "section": "References",
    "text": "References\n\ncolorspace-package:\nhttps://colorspace.r-forge.r-project.org/index.html\nknitr-package:\nhttps://yihui.org/knitr/\nsf-package:\nhttps://r-spatial.github.io/sf/\nrnaturalearth-package:\nhttps://docs.ropensci.org/rnaturalearth/\nggpattern-package:\nhttps://coolbutuseless.github.io/package/ggpattern/index.html\nshowtext-package:\nhttps://cran.r-project.org/web/packages/showtext/index.html\nggtext--package: https://wilkelab.org/ggtext/"
  },
  {
    "objectID": "posts/post-offices.html",
    "href": "posts/post-offices.html",
    "title": "US Post Offices",
    "section": "",
    "text": "## Tidy Tuesday\nThis is my second contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter."
  },
  {
    "objectID": "posts/post-offices.html#us-post-office-data-set",
    "href": "posts/post-offices.html#us-post-office-data-set",
    "title": "US Post Offices",
    "section": "US Post Office Data Set",
    "text": "US Post Office Data Set\nThis week’s data originates from Cameron Blevins and Richard W. Helbock. It covers information on US Post Offices established between 1639 and 2000. It allows to track the development of Post Offices in the US over time.\n\nUS Post Offices Scatter\nOne could obtain a first idea about the data by regarding a scatter plot of the Post Offices over the whole time horizon:\n\n\n\n\n\nGenerally, more post offices are located in the east of the US than in the west. \\(32\\%\\) of the points are randomly located within their corresponding county, because no exact geocoding is available. Because it is more important here to illustrate the number of observations and the rough locational distribution, we do include the randomly located offices.\n\n\nNumber of established Post Offices in each state\nTo gain a further overview over the overall distribution of offices, I first regard the number of established Post Offices in each state:\n\n\n\n\n\nThe coordinates of the states’ labels would need some more fixing. It is apparent that the majority of post offices is accumulated in the coastal regions and densely populated areas.\n\n\nDensity of US Post Offices Along railroads\nRailroads played an important role in the context of expansion of colonies especially in the 19th century. The following plot demonstrates this.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nThe highest density of Post Offices is close to good infrastructure in terms of railroads."
  },
  {
    "objectID": "posts/post-offices.html#references",
    "href": "posts/post-offices.html#references",
    "title": "US Post Offices",
    "section": "References",
    "text": "References\nMain data set:\nBlevins, Cameron; Helbock, Richard W., 2021, “US Post Offices”, https://doi.org/10.7910/DVN/NUKCNA, Harvard Dataverse, V1, UNF:6:8ROmiI5/4qA8jHrt62PpyA== [fileUNF]\nRailroad network:\nJeremy Atack, “Historical Geographic Information Systems (GIS) database of U.S. Railroads for 1848 to 1895”\n\nFull R code available at https://github.com/jgranna/tidytuesday"
  },
  {
    "objectID": "posts/netflix-shows.html",
    "href": "posts/netflix-shows.html",
    "title": "Netflix shows and movies",
    "section": "",
    "text": "This is my third contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom.\n\n\nNetflix provides an increasing number of movies and TV shows in their data base. In order to gain insights by how much the total number of TV shows and movies has increased and whether the TV show / movie overall ratio has changed, I provide the following plot. For this week’s data I use the gggibbous package which includes so-called “moon graphs”.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(gggibbous)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(ggtext)\nnetflix <- netflix$netflix_titles %>% \n  drop_na(date_added)\nknitr::opts_chunk$set(fig.width=12, fig.height=6) \nnetflix$year_added <- as.integer(substr(\n  netflix$date_added, \n  start = nchar(netflix$date_added) - 3,\n  stop  = nchar(netflix$date_added)\n))\nnetflix <- netflix %>% filter(year_added > 2015 & year_added < 2021)\nmoons <- data.frame(\n    y = 1, \n    year = c(sort(unique(netflix$year_added)), sort(unique(netflix$year_added))),  \n    shows = 0, \n    movies = 0\n  )\nfor (i in min(netflix$year_added):max(netflix$year_added)) {\n  moons$movies[moons$year == i] <- table(netflix$type[netflix$year_added == i])[1]\n  moons$shows[moons$year == i]  <- table(netflix$type[netflix$year_added == i])[2]\n}\nmoons$sum   <- moons$shows + moons$shows\nmoons$ratio <- moons$shows/(moons$movies+moons$shows)\nmoons$ratio[6:10] <- 1 - moons$ratio[1:5]\nmoons$right <- rep(c(TRUE, FALSE), each = 5)\nmoons$year_t <- c(2016, 2018, 2020.7, 2024.3, 2028.5)\n\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe number of added movies and TV shows has increased over the years. For all years, Netflix added more movies than TV shows."
  },
  {
    "objectID": "posts/netflix-shows.html#references",
    "href": "posts/netflix-shows.html#references",
    "title": "Netflix shows and movies",
    "section": "References",
    "text": "References\n\nData Set:\nThis week’s data set comes from kaggle.\ngggibbous-package:\nhttps://github.com/mnbram/gggibbous\ntidyverse-package:\nhttps://www.tidyverse.org/\nshowtext-package:\nhttps://github.com/yixuan/showtext\nggtext-package:\nhttps://wilkelab.org/ggtext/\nscales-package:\nhttps://scales.r-lib.org/\n\n\nFull R code available at https://github.com/jgranna/tidytuesday/tree/main/2021-04-20"
  },
  {
    "objectID": "posts/super-mario.html",
    "href": "posts/super-mario.html",
    "title": "Super Mario Kart N64",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load(\"2021-05-25\")"
  },
  {
    "objectID": "posts/super-mario.html#tidy-tuesday",
    "href": "posts/super-mario.html#tidy-tuesday",
    "title": "Super Mario Kart N64",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my sixth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/super-mario.html#super-mario-kart",
    "href": "posts/super-mario.html#super-mario-kart",
    "title": "Super Mario Kart N64",
    "section": "Super Mario Kart",
    "text": "Super Mario Kart\nThis week’s data comes from Mario Kart World Records. It contains world records for the corresponding racing game for the Nintendo 64 gaming console.\n\nConvergence of record times towards their global best\nWhen first regarding the data, I find that more interesting than the absolute record times on the tracks are the relative improvements that have been made on the tracks, and thus, how much more room for improvement there’s probably left. Again, the approach chosen by me is likely to be not the best possible, but the one I find interesting when regarding the data.\nTo obtain interpretable and comparable graphs, I restrict myself to record times that exclude shortcuts and times on PAL systems. For more information on the latter, see the link to the data above. Further, I receive the tracks’ current all-time records and divide the records by the global record to obtain a ratio:\n\n# filter time w/o shortcuts\nrecords <- records %>%\n  filter(shortcut == \"No\") %>%\n  filter(system_played == \"PAL\") %>%\n  filter(type == \"Three Lap\") %>%\n  mutate(image = \"_images/cut.png\")\n# get minimum time for each track\nmins <- aggregate(records$time ~ records$track, FUN = min)\nnames(mins) <- c(\"track\", \"time\")\n# divide time by minimum time\ntracks <- unique(records$track)\nfor (i in 1:length(unique(records$track))) {\n  records$time[records$track == tracks[i]] <- \n    records$time[records$track == tracks[i]] / mins$time[mins$track == tracks[i]]\n}\n\nThen, I plot the records on all tracks:\n\n\n\n\n\nNote: The font and colors are chosen to match the topic, not to be most suitable for interpretation.\nThe tracks in the plot above are hard to distinguish. Thus, I choose to further provide a facet wrap plot which makes the tracks’ records easier to distinguish:\n\n\n\n\n\nIt is apparant that record times on some tracks, like DK’s Jungle Parkway, Yoshi Valley, or Wario Stadium improved quite substantially, while other times only got beat by rather small relative margins. Also, for most tracks, the rate of improvement seems to decline, which is of course not surprising. However, some tracks’ records definitely seem to provide more room for improvement than others."
  },
  {
    "objectID": "posts/super-mario.html#references",
    "href": "posts/super-mario.html#references",
    "title": "Super Mario Kart N64",
    "section": "References",
    "text": "References\n\nggtext-package:\nhttps://wilkelab.org/ggtext/\n\npatchwork-package:\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\nRokemon-package:\nhttps://github.com/schochastics/rokemon\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/\n\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/independence-workdays.html",
    "href": "posts/independence-workdays.html",
    "title": "Independence Weekdays",
    "section": "",
    "text": "holiday <- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-06/holidays.csv\")"
  },
  {
    "objectID": "posts/independence-workdays.html#tidy-tuesday",
    "href": "posts/independence-workdays.html#tidy-tuesday",
    "title": "Independence Weekdays",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my eighth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/independence-workdays.html#independence-days",
    "href": "posts/independence-workdays.html#independence-days",
    "title": "Independence Weekdays",
    "section": "Independence Days",
    "text": "Independence Days\nThis week’s data comes from Wikipedia and was gathered by Isabella Velasquez. It contains the dates of countries when they declared independence.\n\nDay of the week\nI plot the independence days depending on the day of the week as a bar plot:\n\n\n\n\n\nIt is quite (or little?) surprising that there appears to be a pattern for the weekdays of independence declarations: Work days appear generally less “busy” than Friday and Saturday, but there are clearly less independence declarations on Sundays."
  },
  {
    "objectID": "posts/independence-workdays.html#distribution-over-countries",
    "href": "posts/independence-workdays.html#distribution-over-countries",
    "title": "Independence Weekdays",
    "section": "Distribution over countries",
    "text": "Distribution over countries\nThen, I plot the day of the week over the different countries. This time, I want to try the mapsf-package:\n\n\n\n\n\nOver the countries, there does not appear to be a particular pattern regarding which weekday differing countries prefer to declare independence. Just a side note: I prefer using the sf-package over the mapsf package as its usage is more oriented at the usage of ggplot / tidyr.\n\nFull R code available on Github."
  },
  {
    "objectID": "posts/independence-workdays.html#references",
    "href": "posts/independence-workdays.html#references",
    "title": "Independence Weekdays",
    "section": "References",
    "text": "References\n\ncolorspace-package:\nhttps://colorspace.r-forge.r-project.org/index.html\nggtext-package:\nhttps://wilkelab.org/ggtext/\nmapsf-package:\nhttps://riatelab.github.io/mapsf/index.html\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/"
  },
  {
    "objectID": "posts/manager-salaries.html",
    "href": "posts/manager-salaries.html",
    "title": "Manager Salaries",
    "section": "",
    "text": "data <- tidytuesdayR::tt_load(\"2021-05-18\")"
  },
  {
    "objectID": "posts/manager-salaries.html#tidy-tuesday",
    "href": "posts/manager-salaries.html#tidy-tuesday",
    "title": "Manager Salaries",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\nThis is my fifth contribution to TidyTuesday, which is ‘a weekly podcast and community activity brought to you by the R4DS Online Learning Community’. Their goal is to help R learners learn in real-world contexts.\nFor more information, visit the TidyTuesday homepage, check out their GitHub repository and follow the R4DS Learning Community on Twitter.\nThe purpose of these posts is mainly for exercising purposes. Thus, the provided graphs are not necessarily designed to provide the greatest possible insights. However, I always provide the R code for interested people at the page bottom."
  },
  {
    "objectID": "posts/manager-salaries.html#ask-a-manager-survey",
    "href": "posts/manager-salaries.html#ask-a-manager-survey",
    "title": "Manager Salaries",
    "section": "Ask a Manager Survey",
    "text": "Ask a Manager Survey\nThis week’s data comes from Ask a Manager Survey.\n\nInfluence of Experience on annual salary\nOne could expect that the more experience a manager has in the relevant field, the higher her salary will be. I thus provide a bivariate descriptive plot to gain some first insights. To see the distribution in each class in more detail, I provide violin plots.\n\n\n\n\n\nIn general, as expected, years of experience seems to be positively associated with annual salaries of managers."
  },
  {
    "objectID": "posts/manager-salaries.html#references",
    "href": "posts/manager-salaries.html#references",
    "title": "Manager Salaries",
    "section": "References",
    "text": "References\n\nggtext-package:\nhttps://wilkelab.org/ggtext/\nggforce-package:\nhttps://ggforce.data-imaginist.com/\nshowtext-package:\nhttps://github.com/yixuan/showtext\ntidyverse-package:\nhttps://www.tidyverse.org/\n\n\nFull R code available on Github."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Granna, Lang, 2023. Automated Variable Selection on German Real Estate Data\nUsing Structured Additive Distributional Regression. Workshop: Residential Housing Markets: Connecting Housing Researchers in Austria. Abstract\nGranna, Brunauer, Lang, 2022. Proposing a global model to manage the bias-variance tradeoff in the context of hedonic house price models. Working Papers in Economics and Statistics. Link\nGranna, Brunauer, Lang, 2022. Proposing a global model to overcome the bias-variance tradeoff in the context of hedonic house price models. ERES Conference 2022, Milano, Italy.\nGranna, Brunauer, Lang, 2021. Reducing bias of hedonic house price indices. ERES Conference 2021, Kaiserslautern, Germany.\nGranna, 2019. Comparing hedonic house price indices. Master thesis."
  },
  {
    "objectID": "research.html#teaching",
    "href": "research.html#teaching",
    "title": "Research",
    "section": "Teaching",
    "text": "Teaching\n\nPS Statistical Data Analysis (undergraduate level)."
  },
  {
    "objectID": "research.html#co-advising-and-contributions-to-theses",
    "href": "research.html#co-advising-and-contributions-to-theses",
    "title": "Research",
    "section": "(Co-)Advising and contributions to theses",
    "text": "(Co-)Advising and contributions to theses\n\nArmstrong, 2022. Explorations in Generalized Additive and Distributional Regression Modeling in the Bavarian Housing Market Context. Master thesis. Link\nLoos, 2022. Modellierung von Kaufpreisen von Wohnungen in Sachsen in Abhängigkeit von Charakteristika und Lagevariablen. Bachelor thesis.\nWetscher, 2021. Cluster analysis of Berlin rental prices. Master thesis. Link"
  },
  {
    "objectID": "research.html#talks-research-stays-workshops",
    "href": "research.html#talks-research-stays-workshops",
    "title": "Research",
    "section": "Talks, Research Stays & Workshops",
    "text": "Talks, Research Stays & Workshops\n\nJuly 2022 : “Deep Learning” Summer School at Tinbergen Institute, Amsterdam, Netherlands.\n4 March 2022: PhD seminar series, joint program of JKU Linz and University of Innsbruck.\nAugust - September 2021 : Student Exchange at Institute of Agricultural Economics Virginia Tech, USA."
  },
  {
    "objectID": "research.html#scholarships",
    "href": "research.html#scholarships",
    "title": "Research",
    "section": "Scholarships",
    "text": "Scholarships\n\n2022: Stipendium für Kurzfristige Wissenschaftliche Aufenthalte im Ausland (KWA), 2400 Euro."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "rprojects.html",
    "href": "rprojects.html",
    "title": "R Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2022\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\nDevelopment of Chinook Salmon and Yellow Perch populations in the Great Lakes\n\n\n\n\n\n\n\n\n\nJun 8, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nJulian Granna\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2021\n\n\nJulian Granna\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "Simulations",
    "section": "",
    "text": "by\n\nPhilipp Aschersleben\nJulian Granna\nThomas Kneib\nStefan Lang\nNikolaus Umlauf and\nWinfried Steiner\n\nThis page contains accompanying online materials to the paper.\n\n\nWe carry out a simulation study for a Gaussian distribution, where we link the \\(\\mu\\)-parameter to the predictor $$ = f_1(t) + f_2(x) , exp(_{0i} + f_3(z)) + f_4(lon,, lat)\n$$ and the link function is identity. We add noise that is normally distributed with mean 0 and standard deviation equal to \\(\\eta / 2\\). \\(f_1(t)=cos(2\\pi(t-1)/100)\\) with \\(t\\) being a sequence from 1 through 100. \\(f_2(x) = sin(x)\\), where \\(x\\) is uniformly distributed on \\([\\pi /2, \\, 3\\pi /2]\\). \\(f_3(z)=0.7 \\, cos(2\\pi(z-1)/100)\\), where \\(z\\) is uniformly distributed on the interval \\([1,\\, 100]\\). \\(\\gamma_{0i}\\) is a normally distributed cluster-specific random effect for 3-digit postcode regions in Bavaria, Germany, with mean \\(0\\) and standard deviation \\(0.49\\). Finally, \\(f_4(lon,\\, lat)=sin(2lon+lat)\\), where \\(lon\\) and \\(lat\\) correspond to the longitude and latitude coordinates of the centroids of the 3-digit postcode regions.\n\n\n\nUsing the data generating process described above, we simulate 250 sets and run a model on each set. Below, you find the resulting effects as GIFs for each of the 250 models.\n\n\n\n\n\n\n ### Estimated effect \\(\\gamma_{0i}\\)"
  }
]